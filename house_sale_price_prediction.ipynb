{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14863133,"sourceType":"datasetVersion","datasetId":9507757}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =================================================\n# COMPLETE HOUSE PRICE PREDICTION - BASELINE MODEL\n# =================================================\n\n# Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ============================================\n# STEP 1: LOAD DATA\n# ============================================\nprint(\"STEP 1: Loading data...\")\ntrain = pd.read_csv('/kaggle/input/datasets/shadalishah/house-prices-advanced-regression-techniques/train.csv')\nprint(f\" Data loaded: {train.shape[0]} rows, {train.shape[1]} columns\\n\")\n\n# ============================================\n# STEP 2: ANALYZE TARGET VARIABLE\n# ============================================\nprint(\"STEP 2: Analyzing SalePrice...\")\nprint(train['SalePrice'].describe())\nprint(f\"Skewness: {train['SalePrice'].skew():.2f}\\n\")\n\n# ============================================\n# STEP 3: CHECK MISSING VALUES\n# ============================================\nprint(\"STEP 3: Checking missing values...\")\nmissing = train.isnull().sum()\nmissing = missing[missing > 0].sort_values(ascending=False)\nprint(\"Columns with missing values:\")\nprint(missing)\nprint(f\"\\nTotal columns with missing data: {len(missing)}\\n\")\n\n# ============================================\n# STEP 4: HANDLE MISSING VALUES\n# ============================================\nprint(\"STEP 4: Handling missing values...\")\ntrain_clean = train.copy()\n\n# Get column types\nnumerical_cols = train_clean.select_dtypes(include=[np.number]).columns\ncategorical_cols = train_clean.select_dtypes(include=['object']).columns\n\n# Fill missing values\nfor col in numerical_cols:\n    if train_clean[col].isnull().sum() > 0:\n        train_clean[col].fillna(train_clean[col].median(), inplace=True)\n\nfor col in categorical_cols:\n    if train_clean[col].isnull().sum() > 0:\n        train_clean[col].fillna('None', inplace=True)\n\nprint(\" Missing values handled!\\n\")\n\n# ============================================\n# STEP 5: FEATURE ENGINEERING\n# ============================================\nprint(\"STEP 5: Creating new features...\")\ntrain_clean['TotalSF'] = train_clean['TotalBsmtSF'] + train_clean['1stFlrSF'] + train_clean['2ndFlrSF']\ntrain_clean['TotalBath'] = (train_clean['FullBath'] + \n                             (0.5 * train_clean['HalfBath']) + \n                             train_clean['BsmtFullBath'] + \n                             (0.5 * train_clean['BsmtHalfBath']))\ntrain_clean['HouseAge'] = train_clean['YrSold'] - train_clean['YearBuilt']\ntrain_clean['RemodAge'] = train_clean['YrSold'] - train_clean['YearRemodAdd']\n\nprint(\" New features created: TotalSF, TotalBath, HouseAge, RemodAge\\n\")\n\n# ============================================\n# STEP 6: ENCODE CATEGORICAL VARIABLES\n# ============================================\nprint(\"STEP 6: Encoding categorical variables...\")\ntrain_encoded = train_clean.copy()\nle = LabelEncoder()\n\nfor col in categorical_cols:\n    train_encoded[col] = le.fit_transform(train_encoded[col].astype(str))\n\nprint(f\" Encoded {len(categorical_cols)} categorical variables\\n\")\n\n# ============================================\n# STEP 7: PREPARE FEATURES AND TARGET\n# ============================================\nprint(\"STEP 7: Preparing data for modeling...\")\nX = train_encoded.drop(['Id', 'SalePrice'], axis=1)\ny = np.log1p(train_encoded['SalePrice'])  # Log transform\n\nprint(f\"Features shape: {X.shape}\")\nprint(f\"Target shape: {y.shape}\\n\")\n\n# ============================================\n# STEP 8: TRAIN-VALIDATION SPLIT\n# ============================================\nprint(\"STEP 8: Splitting data...\")\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Training set: {X_train.shape}\")\nprint(f\"Validation set: {X_val.shape}\\n\")\n\n# ============================================\n# STEP 9: TRAIN LINEAR REGRESSION MODEL\n# ============================================\nprint(\"STEP 9: Training Linear Regression model...\")\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(\" Model trained!\\n\")\n\n# ============================================\n# STEP 10: EVALUATE MODEL\n# ============================================\nprint(\"STEP 10: Evaluating model...\")\ny_pred_train = model.predict(X_train)\ny_pred_val = model.predict(X_val)\n\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nrmse_val = np.sqrt(mean_squared_error(y_val, y_pred_val))\n\nprint(\"=\"*50)\nprint(\"MODEL PERFORMANCE\")\nprint(\"=\"*50)\nprint(f\"Training RMSE: {rmse_train:.4f}\")\nprint(f\"Validation RMSE: {rmse_val:.4f}\")\nprint(\"=\"*50)\n\n# ============================================\n# STEP 11: VISUALIZE PREDICTIONS\n# ============================================\nprint(\"\\nSTEP 11: Visualizing predictions...\")\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(y_train, y_pred_train, alpha=0.5)\nplt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\nplt.xlabel('Actual Log(Price)')\nplt.ylabel('Predicted Log(Price)')\nplt.title(f'Training Set (RMSE: {rmse_train:.4f})')\n\nplt.subplot(1, 2, 2)\nplt.scatter(y_val, y_pred_val, alpha=0.5)\nplt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)\nplt.xlabel('Actual Log(Price)')\nplt.ylabel('Predicted Log(Price)')\nplt.title(f'Validation Set (RMSE: {rmse_val:.4f})')\n\nplt.tight_layout()\nplt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T02:31:09.543085Z","iopub.execute_input":"2026-02-17T02:31:09.543466Z","iopub.status.idle":"2026-02-17T02:31:10.084149Z","shell.execute_reply.started":"2026-02-17T02:31:09.543432Z","shell.execute_reply":"2026-02-17T02:31:10.083310Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The gap between training RMSE and Validation RMSE is 0.025 (very small gap)\nThis means your model generalizes well to unseen data, so we apply another advance technique to reduce these small gaps ","metadata":{}},{"cell_type":"markdown","source":"Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\nrf_model.fit(X_train, y_train)\ny_pred_rf = rf_model.predict(X_val)\nrmse_rf = np.sqrt(mean_squared_error(y_val, y_pred_rf))\nprint(f\"Random Forest RMSE: {rmse_rf:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T02:37:36.059406Z","iopub.execute_input":"2026-02-17T02:37:36.060012Z","iopub.status.idle":"2026-02-17T02:37:37.666994Z","shell.execute_reply.started":"2026-02-17T02:37:36.059974Z","shell.execute_reply":"2026-02-17T02:37:37.666063Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# XGBOOST MODEL\n# ============================================\nprint(\"Training XGBoost model...\")\nfrom xgboost import XGBRegressor\n\nxgb_model = XGBRegressor(\n    n_estimators=1000,\n    learning_rate=0.05,\n    max_depth=3,\n    random_state=42,\n    n_jobs=-1\n)\n\nxgb_model.fit(X_train, y_train)\n\n# Predictions\ny_pred_xgb = xgb_model.predict(X_val)\nrmse_xgb = np.sqrt(mean_squared_error(y_val, y_pred_xgb))\n\nprint(f\"XGBoost RMSE: {rmse_xgb:.4f}\")\nprint(f\"Improvement over Linear Regression: {((0.1559 - rmse_xgb) / 0.1559 * 100):.1f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T02:37:48.111898Z","iopub.execute_input":"2026-02-17T02:37:48.112295Z","iopub.status.idle":"2026-02-17T02:37:49.622758Z","shell.execute_reply.started":"2026-02-17T02:37:48.112263Z","shell.execute_reply":"2026-02-17T02:37:49.622140Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Final Model Comparison","metadata":{}},{"cell_type":"code","source":"# ============================================\n# MODEL COMPARISON TABLE\n# ============================================\nimport pandas as pd\n\nresults = pd.DataFrame({\n    'Model': ['Linear Regression', 'Random Forest', 'XGBoost'],\n    'Validation RMSE': [0.1559, 0.1442, rmse_xgb],  # Will update with XGBoost result\n})\n\nresults['Improvement vs Baseline'] = ((0.1559 - results['Validation RMSE']) / 0.1559 * 100).round(2)\nresults = results.sort_values('Validation RMSE')\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"MODEL PERFORMANCE COMPARISON\")\nprint(\"=\"*60)\nprint(results.to_string(index=False))\nprint(\"=\"*60)\n\n# Visualize\nplt.figure(figsize=(10, 5))\nplt.barh(results['Model'], results['Validation RMSE'], color=['#ff9999', '#66b3ff', '#99ff99'])\nplt.xlabel('Validation RMSE (Lower is Better)')\nplt.title('Model Performance Comparison', fontsize=14, fontweight='bold')\nplt.gca().invert_yaxis()\nfor i, v in enumerate(results['Validation RMSE']):\n    plt.text(v + 0.002, i, f'{v:.4f}', va='center')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T02:39:21.552230Z","iopub.execute_input":"2026-02-17T02:39:21.552568Z","iopub.status.idle":"2026-02-17T02:39:21.729543Z","shell.execute_reply.started":"2026-02-17T02:39:21.552542Z","shell.execute_reply":"2026-02-17T02:39:21.728441Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"XGBoost model validation RMSE is lower comparitevly all two models such as linear regressions and random forest , so XGBoost is win for final sales prediction ","metadata":{}},{"cell_type":"markdown","source":"Now i upload test datsets to predict final sale price of house to apply XGBoost model","metadata":{}},{"cell_type":"code","source":"# load test datasets\nprint(\"STEP 1: Loading data...\")\ntest = pd.read_csv('/kaggle/input/datasets/shadalishah/house-prices-advanced-regression-techniques/test.csv')\nprint(f\" Data loaded: {train.shape[0]} rows, {train.shape[1]} columns\\n\")\nprint(test.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T02:56:06.599623Z","iopub.execute_input":"2026-02-17T02:56:06.600022Z","iopub.status.idle":"2026-02-17T02:56:06.649343Z","shell.execute_reply.started":"2026-02-17T02:56:06.599992Z","shell.execute_reply":"2026-02-17T02:56:06.648413Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Handle missing values\nfor col in test.columns:\n    if test[col].dtype in ['float64', 'int64']:\n        test[col].fillna(test[col].median(), inplace=True)\n    else:\n        test[col].fillna('None', inplace=True)\n\n# Create features\ntest['TotalSF'] = test['TotalBsmtSF'] + test['1stFlrSF'] + test['2ndFlrSF']\ntest['TotalBath'] = test['FullBath'] + 0.5*test['HalfBath'] + test['BsmtFullBath'] + 0.5*test['BsmtHalfBath']\ntest['HouseAge'] = test['YrSold'] - test['YearBuilt']\ntest['RemodAge'] = test['YrSold'] - test['YearRemodAdd']\n\n# Encode categorical\ncategorical_cols = test.select_dtypes(include=['object']).columns\nle = LabelEncoder()\nfor col in categorical_cols:\n    test[col] = le.fit_transform(test[col].astype(str))\n\n# Prepare features\ntest_ids = test['Id'].copy()\nX_test = test.drop(['Id'], axis=1)\n\n# Align columns\nfor col in X_train.columns:\n    if col not in X_test.columns:\n        X_test[col] = 0\nX_test = X_test[X_train.columns]\n\n# Predict\npredictions = np.expm1(xgb_model.predict(X_test))\n\n# Create submission\nsubmission = pd.DataFrame({'Id': test_ids, 'SalePrice': predictions})\n\n# Save to CSV\nsubmission.to_csv('house_sale_price_predictions.csv', index=False)\n\n# Print all predictions\nprint(\"ALL HOUSE PRICE PREDICTIONS\")\nprint(\"=\"*60)\npd.set_option('display.max_rows', None)\nprint(submission)\nprint(\"=\"*60)\nprint(f\"\\n Total houses predicted: {len(submission)}\")\nprint(f\" File saved: house_sale_price_predictions.csv\")\nprint(f\"\\nPrice Statistics:\")\nprint(f\"  Minimum: ${predictions.min():,.2f}\")\nprint(f\"  Maximum: ${predictions.max():,.2f}\")\nprint(f\"  Average: ${predictions.mean():,.2f}\")\nprint(f\"  Median:  ${np.median(predictions):,.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T03:08:36.088975Z","iopub.execute_input":"2026-02-17T03:08:36.089361Z","iopub.status.idle":"2026-02-17T03:08:36.179424Z","shell.execute_reply.started":"2026-02-17T03:08:36.089330Z","shell.execute_reply":"2026-02-17T03:08:36.178435Z"}},"outputs":[],"execution_count":null}]}